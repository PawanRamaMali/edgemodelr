---
title: "Getting Started with edgemodelr"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting Started with edgemodelr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

```{r setup}
library(edgemodelr)
```

## Introduction

The `edgemodelr` package enables you to run large language models locally on your machine using the GGUF format and llama.cpp inference engine. This provides complete privacy and control over your text generation workflows without requiring cloud APIs or internet connectivity.

## Quick Start

The fastest way to get started is using the `edge_quick_setup()` function:

```{r quickstart}
# Download and setup a model in one line
setup <- edge_quick_setup("TinyLlama-1.1B")
ctx <- setup$context

# Generate text
response <- edge_completion(ctx, "Explain what R is in one sentence:")
cat("Response:", response, "\n")

# Clean up memory
edge_free_model(ctx)
```

## Available Models

You can see all pre-configured models:

```{r models}
models <- edge_list_models()
print(models)
```

## Model Download and Loading

### Automatic Download

```{r download}
# Download a specific model
model_path <- edge_download_model(
  model_id = "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF",
  filename = "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
)

# Load the model
ctx <- edge_load_model(model_path, n_ctx = 2048)
```

### Manual Loading

If you already have a GGUF model file:

```{r manual}
ctx <- edge_load_model("/path/to/your/model.gguf", 
                      n_ctx = 2048,        # Context length
                      n_gpu_layers = 0)    # GPU layers (0 = CPU only)
```

## Text Generation

### Basic Completion

```{r completion}
result <- edge_completion(ctx, 
                         prompt = "The benefits of R programming include:",
                         n_predict = 100,      # Max tokens to generate
                         temperature = 0.8,    # Creativity (0.0-1.0)
                         top_p = 0.95)        # Nucleus sampling
cat(result)
```

### Streaming Generation

For real-time token streaming:

```{r streaming}
result <- edge_stream_completion(ctx, "Write a haiku about data science:",
  callback = function(data) {
    if (!data$is_final) {
      cat(data$token)  # Print each token as generated
      flush.console()
    } else {
      cat("\n[Generation complete]\n")
    }
    return(TRUE)  # Continue generation
  }
)
```

## Interactive Chat

Start an interactive chat session:

```{r chat}
edge_chat_stream(ctx, 
  system_prompt = "You are a helpful R programming assistant.",
  max_history = 10,     # Keep last 10 exchanges
  n_predict = 200,      # Max tokens per response
  temperature = 0.7)    # Response creativity
```

## Batch Processing

Process multiple texts efficiently:

```{r batch}
texts <- c(
  "R is a programming language",
  "Python is also popular", 
  "Both are used for data science"
)

# Analyze sentiment for each text
results <- sapply(texts, function(text) {
  prompt <- paste("Sentiment (positive/negative/neutral):", text, "\nSentiment:")
  response <- edge_completion(ctx, prompt, n_predict = 10)
  trimws(response)
})

print(results)
```

## Memory Management

Always free model memory when done:

```{r cleanup}
# Check if model is valid
is_valid_model(ctx)

# Free memory
edge_free_model(ctx)

# Verify it's freed
is_valid_model(ctx)  # Should return FALSE
```

## System Requirements

### Minimum Requirements
- **RAM**: 4GB+ (varies by model size)
- **CPU**: Modern x86_64 processor
- **Storage**: 1GB+ for model files

### Recommended Specs by Model

| Model Size | RAM | Example Models |
|------------|-----|----------------|
| 1B params  | 2GB | TinyLlama |
| 7B params  | 8GB | Llama-2, Mistral |
| 13B params | 16GB| Llama-2-13B |

## Model Sources

Models can be downloaded from:

- [Hugging Face Hub](https://huggingface.co/models?library=gguf)
- [TheBloke's GGUF collections](https://huggingface.co/TheBloke)
- Local GGUF files converted from other formats

## Tips and Best Practices

1. **Start Small**: Begin with TinyLlama-1.1B for testing
2. **Memory Management**: Always call `edge_free_model()` when done
3. **Context Length**: Adjust `n_ctx` based on your use case
4. **Temperature**: Lower values (0.1-0.3) for factual responses, higher (0.7-0.9) for creative text
5. **Batch Processing**: Reuse loaded models for multiple generations

## Troubleshooting

### Common Issues

**"Failed to load model"**
- Ensure the GGUF file exists and isn't corrupted
- Check available RAM vs model requirements
- Verify file permissions

**"Out of memory"**
- Try a smaller model (TinyLlama vs Llama-2-7B)
- Reduce `n_ctx` parameter
- Close other applications

**Slow generation**
- Models run on CPU by default
- Consider quantized models (Q4_K_M, Q5_K_M)
- Ensure sufficient RAM to avoid swapping

For more help, visit the [GitHub repository](https://github.com/PawanRamaMali/edgemodelr).