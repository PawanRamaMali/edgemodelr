% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/edgemodelr-package.R
\docType{package}
\name{edgemodelr-package}
\alias{edgemodelr}
\alias{edgemodelr-package}
\title{edgemodelr: Local Language Model Inference via 'llama.cpp'}
\description{
Enables R users to run large language models locally using GPU Generative Pre-trained Transformer Unified Format model files and 'llama.cpp' as the inference engine. Provides a complete R interface for loading models, generating text completions, and streaming responses in real-time. Supports on-device inference without requiring cloud APIs or internet connectivity, ensuring complete data privacy and control.
}
\details{
The edgemodelr package provides R bindings for local language model inference
using llama.cpp and GGUF model files. This enables completely private,
on-device text generation without requiring cloud APIs or internet connectivity.
}
\section{Main Functions}{

\describe{
\item{\code{\link{edge_load_model}}}{Load a GGUF model file}
\item{\code{\link{edge_completion}}}{Generate text completions}
\item{\code{\link{edge_stream_completion}}}{Stream text generation in real-time}
\item{\code{\link{edge_chat_stream}}}{Interactive chat interface}
\item{\code{\link{edge_quick_setup}}}{One-line model download and setup}
\item{\code{\link{edge_free_model}}}{Release model memory}
}
}

\section{Model Management}{

\describe{
\item{\code{\link{edge_list_models}}}{List available pre-configured models}
\item{\code{\link{edge_download_model}}}{Download models from Hugging Face}
}
}

\section{Getting Started}{


Basic usage workflow:
\enumerate{
\item Download a model: \code{setup <- edge_quick_setup("TinyLlama-1.1B")}
\item Generate text: \code{edge_completion(setup$context, "Hello")}
\item Clean up: \code{edge_free_model(setup$context)}
}

For interactive chat:
\preformatted{
setup <- edge_quick_setup("TinyLlama-1.1B")
edge_chat_stream(setup$context)
}
}

\section{System Requirements}{


\itemize{
\item C++17 compatible compiler
\item Sufficient RAM for model size (1GB+ for small models, 8GB+ for 7B models)
\item GGUF model files (downloaded automatically or manually)
}
}

\section{Privacy and Security}{


This package processes all data locally on your machine. No data is sent to
external servers, ensuring complete privacy and control over your text
generation workflows.
}

\seealso{
\itemize{
\item Package repository: \url{https://github.com/PawanRamaMali/edgemodelr}
\item llama.cpp project: \url{https://github.com/ggml-org/llama.cpp}
\item GGUF format: \url{https://github.com/ggml-org/ggml}
}
}
\author{
Pawan Rama Mali \email{prm@outlook.in}
}
\keyword{internal}
